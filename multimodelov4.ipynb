{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRoblemas:\n",
    "\n",
    "# 1. Al probar test, no podemos usar columnas de train, porque puede haber hecho hot_encoding. \n",
    "\n",
    "Solucion:\n",
    "\n",
    "1. Aislar en funcion la parte que devuelve las columnas. Si es una columna categorica, la devolvemos.\n",
    "2. Aislar en funcion que obtenga el df_handled. Si es col numerica, normaliza, si es categorica onehot...\n",
    "3. Revisar caso que no quita id (ejemplo de kaggle house prices)\n",
    "4. Probar\n",
    "5. Tras esto, podremos desde la prueba de kaggle, llamar a df_train\n",
    "\n",
    "# 2. Hay casos donde no borra columna Id. Por ejemlo regresion_Avanzada/input que son ejemplos de house prices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "# Regression algorithms\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet # LogisticRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.neural_network import MLPRegressor\n",
    "# from xgboost import XGBRegressor\n",
    "\n",
    "# Classification algorithms\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC # Support Vector Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from xgboost import XGBClassifier\n",
    "# from sklearn import svm\n",
    "# from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "\n",
    "# Utils\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score\n",
    "\n",
    "class ModelType(Enum):\n",
    "    CLASSIFICATION = \"Classification\"\n",
    "    REGRESSION = \"regression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_models = {\n",
    "    'random_forest_classifier': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': [100],  # Número de árboles\n",
    "            'max_depth': [5, 10],  # Profundidad máxima del árbol\n",
    "            'min_samples_split': [5, 10],  # Mínimo de muestras para dividir un nodo\n",
    "            'random_state': [1]\n",
    "        }\n",
    "    },\n",
    "    'logistic_regression': {\n",
    "        'model': LogisticRegression(),\n",
    "        'params': {\n",
    "            'C': [0.1, 1, 10],  # Inversa de la regularización\n",
    "            'solver': ['liblinear', 'lbfgs'],  # Algoritmo de optimización\n",
    "            'max_iter': [100, 200]  # Número máximo de iteraciones\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # 'svm_classifier': {\n",
    "    #     'model': SVC(),\n",
    "    #     'params': {\n",
    "    #         'C': [0.1, 1, 10],  # Penalización del margen\n",
    "    #         'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],  # Tipo de kernel\n",
    "    #         'gamma': ['scale', 'auto']  # Control de la influencia de los puntos\n",
    "    #     }\n",
    "    # },\n",
    "\n",
    "    'knn_classifier': {\n",
    "        'model': KNeighborsClassifier(),\n",
    "        'params': {\n",
    "            'n_neighbors': [3, 5, 10],  # Número de vecinos\n",
    "            'weights': ['uniform', 'distance'],  # Peso de los vecinos\n",
    "            'p': [1, 2]  # Tipo de distancia (Manhattan = 1, Euclidiana = 2)\n",
    "        }\n",
    "    },\n",
    "    'gradient_boosting_classifier': {\n",
    "        'model': GradientBoostingClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200],  # Número de árboles\n",
    "            'learning_rate': [0.01, 0.1, 0.2],  # Tasa de aprendizaje\n",
    "            'max_depth': [3, 5, 10]  # Profundidad de los árboles\n",
    "        }\n",
    "    },\n",
    "    'decision_tree_classifier': {\n",
    "        'model': DecisionTreeClassifier(),\n",
    "        'params': {\n",
    "            'max_depth': [5, 10, 20],  # Profundidad máxima\n",
    "            'min_samples_split': [2, 5, 10],  # Min muestras para dividir nodo\n",
    "            'criterion': ['gini', 'entropy']  # Función de división\n",
    "        }\n",
    "    },\n",
    "    'xgboost_classifier': {\n",
    "        'model': XGBClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200, 300],  # Número de árboles\n",
    "            'learning_rate': [0.01, 0.1, 0.2],  # Tasa de aprendizaje\n",
    "            'max_depth': [3, 5, 10],  # Profundidad de los árboles\n",
    "            'subsample': [0.8, 1.0]  # Fracción de muestras usadas en cada árbol\n",
    "        }\n",
    "    }\n",
    "} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_models = {\n",
    "    'linear_regresion': {\n",
    "        'model': LinearRegression(),\n",
    "        'params' : {\n",
    "            'fit_intercept': [True, False],\n",
    "            'positive': [True, False],\n",
    "        }\n",
    "    },\n",
    "    # Reducir overfitting con regularización (Ridge/Lasso)\n",
    "    'ridge_regresion': {\n",
    "        'model': Ridge(),\n",
    "        'params': {\n",
    "            'alpha': [0.1, 1.0, 10.0],  # Controla la penalización L2\n",
    "            'fit_intercept': [True, False]\n",
    "        }\n",
    "    },\n",
    "    'lasso_regresion': {\n",
    "        'model': Lasso(),\n",
    "        'params': {\n",
    "            'alpha': [0.01, 0.1, 1.0],  # Controla la penalización L1\n",
    "            'fit_intercept': [True, False]\n",
    "        }\n",
    "    },\n",
    "    'elastic_net': {\n",
    "        'model': ElasticNet(),\n",
    "        'params': {\n",
    "            'alpha': [0.01, 0.1, 1.0],  # Controla la penalización\n",
    "            'l1_ratio': [0.2, 0.5, 0.8]  # Balance entre L1 (Lasso) y L2 (Ridge)\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # 'svr': {\n",
    "    #     'model': SVR(),\n",
    "    #     'params': {\n",
    "    #         'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    #         'C': [0.1, 1, 10],  # Controla la penalización de margen\n",
    "    #         'epsilon': [0.01, 0.1, 0.5]  # Define el margen de error permitido\n",
    "    #     }\n",
    "    # },\n",
    "    \n",
    "    # 'random_forest': {\n",
    "    #     'model': RandomForestRegressor(),\n",
    "    #     'params': {\n",
    "    #         'n_estimators': [50, 100, 200],  # Número de árboles en el bosque\n",
    "    #         'max_depth': [None, 10, 20],  # Profundidad máxima del árbol\n",
    "    #         'min_samples_split': [2, 5, 10]  # Mínimo de muestras necesarias para dividir un nodo\n",
    "    #     }\n",
    "    # },\n",
    "    \n",
    "    'knn': {\n",
    "        'model': KNeighborsRegressor(),\n",
    "        'params': {\n",
    "            'n_neighbors': [3, 5, 10],  # Número de vecinos a considerar\n",
    "            'weights': ['uniform', 'distance'],  # Cómo se ponderan los vecinos\n",
    "            'p': [1, 2]  # Tipo de distancia: 1=Minkowski (Manhattan), 2=Euclidiana\n",
    "        }\n",
    "    },\n",
    "    'decision_tree': {\n",
    "        'model': DecisionTreeRegressor(),\n",
    "        'params': {\n",
    "            'max_depth': [None, 10, 20],  # Profundidad máxima del árbol\n",
    "            'min_samples_split': [2, 5, 10]  # Mínimo de muestras para dividir un nodo\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ Indicadores de overfitting:\n",
    "\n",
    "1️⃣ Accuracy en entrenamiento (train_accuracy) es mucho mayor que en prueba (val_accuracy)\n",
    "\n",
    "Si train_accuracy es muy alto (99%) y val_accuracy es mucho más bajo (60-70%), el modelo está memorizando en lugar de aprender patrones generales.\n",
    "\n",
    "2️⃣ Diferencia alta en R² Score (para regresión)\n",
    "\n",
    "R²_train es muy alto (~0.95-1.0), pero R²_test es muy bajo (~0.5 o menos) → Overfitting.\n",
    "\n",
    "3️⃣ Modelo complejo con muchos parámetros (ej. demasiados árboles en RandomForest, muchas capas en redes neuronales)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na(df):\n",
    "\n",
    "    # Columnas numericas -> Media\n",
    "    numeric_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
    "    \n",
    "    # Eliminar registros con valores nulos en columnas categóricas\n",
    "    # df = df.dropna(subset=df.select_dtypes(include=['object', 'category']).columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "def identify_type_model(df, target_column):\n",
    "    \"\"\"\n",
    "    Retorna:\n",
    "    - \"Clasificación\" si la variable objetivo tiene pocos valores únicos o es categórica.\n",
    "    - \"Regresión\" si la variable objetivo tiene muchos valores únicos y es numérica.\n",
    "    - Por tanto no tenemos en cuenta si es target es int o float, solo el umbral de valores. \n",
    "    - Si fuera float con 3 valores, seria un problema igualmente de clasificacion.\n",
    "\n",
    "    \"\"\"\n",
    "    umbral_clasificacion=20\n",
    "\n",
    "    num_unique_values = df[target_column].nunique()\n",
    "    target_is_numeric_column = pd.api.types.is_numeric_dtype(df[target_column])\n",
    "\n",
    "    if not target_is_numeric_column:\n",
    "        return ModelType.CLASSIFICATION   # Si es texto o categoría, es clasificación\n",
    "    elif num_unique_values <= umbral_clasificacion:\n",
    "        return ModelType.CLASSIFICATION   # Pocos valores únicos → clasificación\n",
    "    else:\n",
    "        return ModelType.REGRESSION   # Muchos valores únicos → regresión\n",
    "\n",
    "def get_relevant_columns(df, target_column):\n",
    "\n",
    "    correlacion_minima=0.01\n",
    "    umbral_categoricas = 0.05\n",
    "    umbral_unicos = 0.1 * len(df)  # Si una columna tiene más del 10% de valores únicos, se excluye\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    # total_filas = len(df_copy) # 891\n",
    "\n",
    "    # *********************************************************************** #\n",
    "    # ************************* COLUMNAS NUMERICAS ************************** #\n",
    "    # *********************************************************************** #\n",
    "    columnas_numericas = df_copy.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    # Filtrar columnas con demasiados valores únicos, excepto target\n",
    "    columnas_numericas = [\n",
    "        col for col in columnas_numericas \n",
    "        if (df[col].nunique() <= umbral_unicos or col == target_column)  \n",
    "    ]\n",
    "\n",
    "    correlacion_pearson = df_copy[columnas_numericas].corrwith(df[target_column])\n",
    "    correlacion_spearman = df_copy[columnas_numericas].corr(method='spearman')[target_column]\n",
    "\n",
    "    correlaciones = pd.DataFrame({\n",
    "        'Pearson': correlacion_pearson,\n",
    "        'Spearman': correlacion_spearman\n",
    "    })\n",
    "    \n",
    "    columnas_relevantes_numericas = correlaciones[correlaciones.abs().max(axis=1) > correlacion_minima].index.tolist()    \n",
    "\n",
    "    # ************************************************************************* #\n",
    "    # ************************* COLUMNAS CATEGORICAS ************************** #\n",
    "    # ************************************************************************* #\n",
    "    columnas_categoricas = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    columnas_relevantes_categoricas = []\n",
    "    \n",
    "    for col in columnas_categoricas:\n",
    "        # Excluida por tener demasiados valores únicos\n",
    "        if df[col].nunique() > umbral_unicos:\n",
    "            continue\n",
    "\n",
    "        tabla_contingencia = pd.crosstab(df_copy[col], df_copy[target_column])\n",
    "        chi2, p, _, _ = stats.chi2_contingency(tabla_contingencia)\n",
    "        \n",
    "        if p < umbral_categoricas:\n",
    "            columnas_relevantes_categoricas.append(col)\n",
    "\n",
    "    columnas_relevantes = set(columnas_relevantes_numericas+ columnas_relevantes_categoricas)\n",
    "    \n",
    "    return list(columnas_relevantes)\n",
    "\n",
    "def get_df_normalized_numeric_columns(df):\n",
    "\n",
    "    features_numeric_column_names = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized = scaler.fit_transform(df[features_numeric_column_names])\n",
    "    normalized_df_numericas = pd.DataFrame(normalized, columns=df[features_numeric_column_names].columns)\n",
    "\n",
    "    return normalized_df_numericas\n",
    "\n",
    "def get_df_normalized_categoric_columns(df):    \n",
    "\n",
    "    features_categoric_column_names = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    if features_categoric_column_names:\n",
    "        return pd.get_dummies(df[features_categoric_column_names])\n",
    "    else: \n",
    "        return pd.DataFrame()\n",
    "\n",
    "def get_df_handled(df, target_column):\n",
    "    \"\"\"\n",
    "    \n",
    "    Esta funcion prepara el dataframe, quitando columnas no relevantes y normalizando los datos.\n",
    "    - Aplicamos media a nulos numericos\n",
    "    - Quitamos features numericas sin correlacion\n",
    "    - Quitamos features categoricas sin correlacion\n",
    "    - Aplicamos normalizacion a target\n",
    "\n",
    "    *** IMPORTANTE ***\n",
    "    El dataframe de datos debe ser tratado antes de hacer split. \n",
    "    Ya que puede devolver diferentes columnas en funcion de los datos facilitados\n",
    "    \n",
    "    \"\"\"\n",
    "    # Verificar si la columna objetivo es categórica\n",
    "    if df[target_column].dtype == 'object' or df[target_column].dtype.name == 'category':\n",
    "        encoder = LabelEncoder()\n",
    "        df[target_column] = encoder.fit_transform(df[target_column])\n",
    "\n",
    "    # Tratamos los nulos\n",
    "    df = fill_na(df)\n",
    "\n",
    "    # Dejamos solo columnas con cierta correlacion\n",
    "    cols_relevantes = get_relevant_columns(df, target_column)\n",
    "    df = df[cols_relevantes]\n",
    "\n",
    "    # pre-processing\n",
    "    # Debe resetear indice para que luego lo una bien\n",
    "    normalized_df_numericas = get_df_normalized_numeric_columns(df).reset_index(drop=True) \n",
    "    normalized_df_categoricas = get_df_normalized_categoric_columns(df).reset_index(drop=True)\n",
    "\n",
    "    res = pd.concat([normalized_df_categoricas,normalized_df_numericas], axis=1)\n",
    "\n",
    "    return res\n",
    "\n",
    "def get_best_model(df, target_column, debug=False):\n",
    "    \n",
    "    scores = []\n",
    "\n",
    "    _modelType = identify_type_model(df, target_column)    \n",
    "    \n",
    "    if (debug):\n",
    "        print('Model identified:', _modelType.value)\n",
    "        print(\"\")\n",
    "\n",
    "    X = df.drop(target_column, axis=1)    \n",
    "\n",
    "    if _modelType == ModelType.CLASSIFICATION:\n",
    "        df[target_column] = df[target_column].astype(int)\n",
    "        \n",
    "    y = df[target_column]\n",
    "\n",
    "    if _modelType == ModelType.REGRESSION:\n",
    "\n",
    "        for model_name, mp in regression_models.items():\n",
    "            \n",
    "            clf = GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n",
    "\n",
    "            start_time = time.time()  \n",
    "            clf.fit(X, y)\n",
    "            end_time = time.time()  \n",
    "            training_time = round(end_time - start_time)\n",
    "\n",
    "            scores.append({\n",
    "                'model': model_name,                \n",
    "                'best_score': clf.best_score_, # El mejor R² (coeficiente de determinación) obtenido en validación cruzada.\n",
    "                'best_params': clf.best_params_,\n",
    "                'training_time': training_time                \n",
    "            })\n",
    "        \n",
    "            if (debug):\n",
    "                print(f\"Model tested ({training_time}s): {model_name}  best_score: {clf.best_score_}  best_params: {clf.best_params_}\")\n",
    "\n",
    "    else: # Clasificacion\n",
    "\n",
    "        for model_name, mp in classification_models.items():\n",
    "            \n",
    "            clf = GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n",
    "            \n",
    "            start_time = time.time()  \n",
    "            clf.fit(X, y)\n",
    "            end_time = time.time()  \n",
    "            training_time = round(end_time - start_time)\n",
    "\n",
    "            scores.append({\n",
    "                'model': model_name,                \n",
    "                'best_score': clf.best_score_, # Promedio r2\n",
    "                'best_params': clf.best_params_,\n",
    "                'training_time': training_time \n",
    "            })\n",
    "\n",
    "            if (debug):\n",
    "                print(f\"Model tested ({training_time}s): {model_name}  best_score: {clf.best_score_}  best_params: {clf.best_params_}\")\n",
    "\n",
    "    # Obtenemos el primer modelo ordenandolo por best_score_\n",
    "    models_summary = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\n",
    "    models_summary = models_summary.sort_values(by=['best_score'], ascending=False)\n",
    "    best_model_info = models_summary.iloc[0]      \n",
    "\n",
    "    # Leemos detalles modelo\n",
    "    best_model_name = best_model_info['model']\n",
    "    best_params = best_model_info['best_params']\n",
    "    \n",
    "    # Entrenamos y validamos modelo\n",
    "    if _modelType == ModelType.REGRESSION:\n",
    "        best_model = regression_models[best_model_name]['model'].__class__(**best_params)    \n",
    "    else:\n",
    "        best_model = classification_models[best_model_name]['model'].__class__(**best_params)    \n",
    "    \n",
    "    best_model.fit(X, y)\n",
    "    train_accuracy = best_model.score(X, y)    \n",
    "    #print(X)\n",
    "\n",
    "    response = { \n",
    "        'type': _modelType.value,        \n",
    "        'best_model': best_model,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'target_column': target_column,\n",
    "        'columns_relevants': X.columns,\n",
    "        'best_params': best_params            \n",
    "    }\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def print_results(response, X_test, y_test):\n",
    "    \n",
    "    model = response['best_model']\n",
    "    val_accuracy = model.score(X_test, y_test)\n",
    "    \n",
    "    # Should executing score before call class attributes\n",
    "    model_name = response['best_model'].__class__.__name__\n",
    "    \n",
    "    print(\" \")\n",
    "    print(\"Best model name:\", model_name)\n",
    "    print('train_accuracy:', response['train_accuracy'])\n",
    "    print('val_accuracy:' , val_accuracy)    \n",
    "\n",
    "def features_target_split(df, target_column):\n",
    "    \n",
    "    features = df.drop(columns=[target_column]) \n",
    "    target = df[target_column]\n",
    "    return features, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos entrenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model identified: regression\n",
      "\n",
      "Model tested (0s): linear_regresion  best_score: 0.7934803866634184  best_params: {'fit_intercept': True, 'positive': False}\n",
      "Model tested (0s): ridge_regresion  best_score: 0.8209085465529913  best_params: {'alpha': 10.0, 'fit_intercept': False}\n",
      "Model tested (0s): lasso_regresion  best_score: 0.5410620635345875  best_params: {'alpha': 0.01, 'fit_intercept': False}\n",
      "Model tested (0s): elastic_net  best_score: 0.7415891355694738  best_params: {'alpha': 0.01, 'l1_ratio': 0.2}\n",
      "Model tested (1s): knn  best_score: 0.7003748400567605  best_params: {'n_neighbors': 10, 'p': 1, 'weights': 'distance'}\n",
      "Model tested (0s): decision_tree  best_score: 0.5350406042325895  best_params: {'max_depth': None, 'min_samples_split': 10}\n",
      " \n",
      "Best model name: Ridge\n",
      "train_accuracy: 0.852893640116654\n",
      "val_accuracy: 0.8459265663943566\n"
     ]
    }
   ],
   "source": [
    "# Kaggle - House prices\n",
    "df = pd.read_csv(\"input/house_prices/train.csv\")\n",
    "target_column = \"SalePrice\"\n",
    "\n",
    "df = get_df_handled(df, target_column)    \n",
    "\n",
    "features, target = features_target_split(df, target_column) \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data = pd.concat([X_train , y_train], axis=1)\n",
    "response = get_best_model(train_data, target_column, debug=True)\n",
    "\n",
    "print_results(response,X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"input/house_prices/train.csv\")\n",
    "df.head()\n",
    "\n",
    "#target_column = \"SalePrice\"\n",
    "\n",
    "# df = get_df_handled(df, target_column)    \n",
    "# #cols_relevantes = df.columns\n",
    "\n",
    "# df_test = pd.read_csv(\"input/house_prices/test.csv\")\n",
    "# df_test = get_df_handled(df_test, target_column)  \n",
    "\n",
    "#df_test = df_test[cols_relevantes]\n",
    "#columnas_faltantes = [col for col in cols_relevantes if col not in df_test.columns]\n",
    "#print(\"Columnas que no existen en df_test:\", columnas_faltantes)\n",
    "\n",
    "\n",
    "\n",
    "# Dejamos solo columnas con cierta correlacion\n",
    "#cols_relevantes = get_relevant_columns(df_test, target_column)\n",
    "# 45 columnas\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df_test.columns = 80\n",
    "\n",
    "#cols_train = list(features.columns)\n",
    "#df_test_clean = df_test[cols_train]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
